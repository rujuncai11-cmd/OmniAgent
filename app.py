"""
Streamlit RAG Web UI (app.py)
å®Œæ•´çš„ Web åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š
- èŠå¤©ç•Œé¢
- å®æ—¶æ£€ç´¢æ˜¾ç¤º
- å‚æ•°è°ƒæ•´
- æ€§èƒ½ç»Ÿè®¡
- å¯¹è¯å†å²ä¿å­˜

è¿è¡Œï¼šstreamlit run app.py
"""

import streamlit as st
import time
import os
from pathlib import Path
from datetime import datetime
import json

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ======================== é…ç½® ========================
KNOWLEDGE_BASE_PATH = r"D:\HF_models\knowledge_base"
FAISS_INDEX_PATH = r"D:\HF_models\faiss_index"
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
CACHE_DIR = r"D:\HF_models"
CHAT_HISTORY_PATH = "chat_history.json"

# ======================== é¡µé¢é…ç½® ========================
st.set_page_config(
    page_title="ğŸ¤– AI ç ”ç©¶åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS
st.markdown("""
<style>
    .stChat {
        background-color: #f0f2f6;
    }
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 0.5rem;
    }
    .source-box {
        background-color: #e8f4f8;
        padding: 1rem;
        border-left: 4px solid #0066cc;
        margin-top: 0.5rem;
        border-radius: 0.25rem;
    }
    .stats-box {
        background-color: #fff4e6;
        padding: 0.8rem;
        border-radius: 0.25rem;
        font-size: 0.9rem;
    }
</style>
""", unsafe_allow_html=True)

# ======================== Session State åˆå§‹åŒ– ========================
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.vector_store = None
    st.session_state.total_tokens = 0

# ======================== åŠ è½½æ¨¡å‹å’Œå‘é‡åº“ ========================
@st.cache_resource
def load_models():
    """ç¼“å­˜åŠ è½½æ¨¡å‹"""
    with st.spinner("â³ åŠ è½½æ¨¡å‹ä¸­..."):
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            cache_dir=CACHE_DIR
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            cache_dir=CACHE_DIR
        )
        
        # åŠ è½½å‘é‡åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={"device": "cuda:0"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        vector_store = FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
        
        st.session_state.model = model
        st.session_state.tokenizer = tokenizer
        st.session_state.vector_store = vector_store
        
        return tokenizer, model, vector_store

# ======================== RAG æ£€ç´¢å‡½æ•° ========================
def retrieve_documents(query: str, top_k: int = 5):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    retrieve_start = time.time()
    
    vector_store = st.session_state.vector_store
    docs = vector_store.similarity_search(query, k=top_k)
    
    retrieve_time = time.time() - retrieve_start
    
    return docs, retrieve_time

# ======================== ç”Ÿæˆç­”æ¡ˆå‡½æ•° ========================
def generate_answer(query: str, documents, temperature: float, top_p: float):
    """ç”Ÿæˆç­”æ¡ˆ"""
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([
        f"ã€æ¥æºï¼š{doc.metadata.get('filename', 'Unknown')}ã€‘\n{doc.page_content}"
        for doc in documents[:3]  # åªç”¨å‰ 3 ç¯‡
    ])
    
    # æ„å»º Prompt
    prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI ç ”ç©¶åŠ©æ‰‹ï¼Œç°åœ¨éœ€è¦æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- ç›´æ¥ã€ç®€æ´åœ°å›ç­”é—®é¢˜
- åŸºäºçŸ¥è¯†åº“ä¿¡æ¯
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´"æš‚æ— ç›¸å…³ä¿¡æ¯"
- é•¿åº¦ï¼š100-300 å­—

ã€ç­”æ¡ˆã€‘"""
    
    # ç”Ÿæˆ
    generate_start = time.time()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=temperature,
        top_p=top_p,
        do_sample=True,
        repetition_penalty=1.2
    )
    
    answer = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )
    
    generate_time = time.time() - generate_start
    
    return answer.strip(), generate_time

# ======================== åŠ è½½å¯¹è¯å†å² ========================
def load_chat_history():
    """åŠ è½½å¯¹è¯å†å²"""
    if os.path.exists(CHAT_HISTORY_PATH):
        with open(CHAT_HISTORY_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

# ======================== ä¿å­˜å¯¹è¯å†å² ========================
def save_chat_history():
    """ä¿å­˜å¯¹è¯å†å²"""
    with open(CHAT_HISTORY_PATH, 'w', encoding='utf-8') as f:
        json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

# ======================== ä¸»åº”ç”¨ ========================
def main():
    # å¤´éƒ¨
    st.title("ğŸ¤– AI ç ”ç©¶åŠ©æ‰‹")
    st.markdown("åŸºäº RAG + Qwen 3B çš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # æ¨¡å‹å‚æ•°
        st.subheader("ç”Ÿæˆå‚æ•°")
        temperature = st.slider(
            "æ¸©åº¦ (Temperature)",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1,
            help="è¶Šä½è¶Šç¡®å®šï¼Œè¶Šé«˜è¶Šéšæœº"
        )
        
        top_p = st.slider(
            "æ¦‚ç‡é˜ˆå€¼ (Top P)",
            min_value=0.0,
            max_value=1.0,
            value=0.8,
            step=0.1,
            help="åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡åœ¨æ­¤é˜ˆå€¼å†…çš„ token"
        )
        
        top_k = st.slider(
            "æ£€ç´¢æ–‡æ¡£æ•° (Top K)",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="æ£€ç´¢å¤šå°‘ç¯‡ç›¸å…³æ–‡æ¡£"
        )
        
        # æ¨¡å‹ä¿¡æ¯
        st.subheader("ğŸ’» ç³»ç»Ÿä¿¡æ¯")
        
        if st.button("ğŸ”„ åˆå§‹åŒ–æ¨¡å‹", key="load_btn"):
            load_models()
            st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        if st.session_state.model is not None:
            st.write("âœ… æ¨¡å‹å·²åŠ è½½")
            
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1e9
                total = torch.cuda.get_device_properties(0).total_memory / 1e9
                st.write(f"ğŸ’¾ æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
        else:
            st.write("âŒ æ¨¡å‹æœªåŠ è½½")
        
        # å¯¹è¯å†å²
        st.subheader("ğŸ“ å¯¹è¯å†å²")
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", key="clear_btn"):
            st.session_state.messages = []
            save_chat_history()
            st.success("âœ… å¯¹è¯å·²æ¸…ç©º")
        
        if st.button("ğŸ’¾ ä¿å­˜å¯¹è¯", key="save_btn"):
            save_chat_history()
            st.success("âœ… å¯¹è¯å·²ä¿å­˜")
        
        if st.button("ğŸ“‚ åŠ è½½å¯¹è¯", key="load_history_btn"):
            history = load_chat_history()
            if history:
                st.session_state.messages = history
                st.success(f"âœ… åŠ è½½äº† {len(history)} æ¡æ¶ˆæ¯")
            else:
                st.info("â„¹ï¸ æ²¡æœ‰ä¿å­˜çš„å¯¹è¯å†å²")
    
    # ä¸»å†…å®¹åŒº
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ’¬ å¯¹è¯")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.write(message["content"])
                    if "sources" in message and message["sources"]:
                        with st.expander(f"ğŸ“š æ¥æº ({len(message['sources'])} ç¯‡)"):
                            for source in message["sources"]:
                                st.markdown(
                                    f"<div class='source-box'>"
                                    f"<strong>ğŸ“„ {source['filename']}</strong><br>"
                                    f"{source['preview'][:200]}..."
                                    f"</div>",
                                    unsafe_allow_html=True
                                )
                    if "stats" in message:
                        with st.expander("â±ï¸ æ€§èƒ½ç»Ÿè®¡"):
                            st.markdown(
                                f"<div class='stats-box'>"
                                f"ğŸ“Š æ£€ç´¢è€—æ—¶: {message['stats']['retrieve_ms']:.0f}ms<br>"
                                f"ğŸ¤– ç”Ÿæˆè€—æ—¶: {message['stats']['generate_ms']:.0f}ms<br>"
                                f"â±ï¸ æ€»è€—æ—¶: {message['stats']['total_s']:.2f}s"
                                f"</div>",
                                unsafe_allow_html=True
                            )
    
    with col2:
        st.subheader("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        st.metric("å¯¹è¯æ•°", len(st.session_state.messages))
        st.metric("æ€» tokens", st.session_state.total_tokens)
    
    # è¾“å…¥æ¡†
    st.markdown("---")
    user_input = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜...")
    
    if user_input:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if st.session_state.model is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§ç‚¹å‡» 'åˆå§‹åŒ–æ¨¡å‹' æŒ‰é’®")
        else:
            # ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({
                "role": "user",
                "content": user_input
            })
            
            with st.chat_message("user"):
                st.write(user_input)
            
            # å¤„ç†ç”¨æˆ·è¾“å…¥
            with st.spinner("ğŸ” æ£€ç´¢ä¸­..."):
                total_start = time.time()
                
                # æ£€ç´¢
                docs, retrieve_time = retrieve_documents(user_input, top_k=top_k)
                
                # ç”Ÿæˆ
                answer, generate_time = generate_answer(
                    user_input, docs, temperature, top_p
                )
                
                total_time = time.time() - total_start
            
            # åŠ©æ‰‹æ¶ˆæ¯
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "sources": [
                    {
                        "filename": doc.metadata.get('filename', 'Unknown'),
                        "preview": doc.page_content[:100]
                    }
                    for doc in docs
                ],
                "stats": {
                    "retrieve_ms": retrieve_time * 1000,
                    "generate_ms": generate_time * 1000,
                    "total_s": total_time
                }
            })
            
            # æ›´æ–°ç»Ÿè®¡
            st.session_state.total_tokens += len(
                st.session_state.tokenizer.encode(user_input + answer)
            )
            
            # ä¿å­˜å¯¹è¯
            save_chat_history()
            
            # åˆ·æ–°é¡µé¢æ˜¾ç¤º
            st.rerun()

if __name__ == "__main__":
    main()