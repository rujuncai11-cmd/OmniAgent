"""
âœ… å®Œæ•´çš„ RAG ç³»ç»Ÿï¼ˆPDF + 3B æ¨¡å‹ï¼‰- ä¿®å¤ç‰ˆ
- æ­£ç¡®çš„åº“å¯¼å…¥è·¯å¾„
- åŠ è½½ PDF æ–‡ç« 
- æ„å»ºå‘é‡ç´¢å¼•
- åŸºäº Qwen2.5-3B çš„é—®ç­”ç³»ç»Ÿ
- å¿…é¡»è¿”å›å‡ºå¤„
"""

import os
import sys
import time
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict

print("\n" + "="*70)
print("ğŸ“š RAG ç³»ç»Ÿåˆå§‹åŒ–")
print("="*70)

# ============ Step 1: å®‰è£…å’Œå¯¼å…¥æ‰€éœ€åº“ ============
print("\n1ï¸âƒ£  å¯¼å…¥åº“...")

# å…ˆæ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åº“
required_libs = {
    'PyPDF2': 'PyPDF2',
    'sentence_transformers': 'sentence-transformers',
    'faiss': 'faiss-cpu',
}

for lib_import, lib_install in required_libs.items():
    try:
        __import__(lib_import)
        print(f"   âœ“ {lib_import}")
    except ImportError:
        print(f"   âš ï¸  {lib_import} æœªå®‰è£…ï¼Œå®‰è£…ä¸­...")
        os.system(f"pip install {lib_install} -q")
        print(f"   âœ“ {lib_import} å·²å®‰è£…")

# å¯¼å…¥åº“
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer

print("   âœ“ transformers")
print("   âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ")

# ============ Step 2: åŠ è½½ Qwen2.5-3B æ¨¡å‹ ============
print("\n2ï¸âƒ£  åŠ è½½ Qwen2.5-3B æ¨¡å‹...")

model_id = "Qwen/Qwen2.5-3B-Instruct"
cache_dir = r"D:\HF_models"

try:
    print("   åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    print("   âœ“ tokenizer åŠ è½½æˆåŠŸ")
    
    print("   åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    model.eval()
except Exception as e:
    print(f"   âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# ============ Step 3: PDF åŠ è½½å’Œæ–‡æœ¬æå– ============
print("\n3ï¸âƒ£  åŠ è½½ PDF æ–‡ç« ...")

class PDFArticleLoader:
    """åŠ è½½ PDF æ–‡ç« å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯"""
    
    def __init__(self, pdf_folder: str):
        self.pdf_folder = pdf_folder
        self.documents = []
    
    def load_pdfs(self) -> List[Dict]:
        """åŠ è½½æ‰€æœ‰ PDF æ–‡ä»¶"""
        # å¤šç§æ–¹å¼æŸ¥æ‰¾ PDF
        pdf_files = []
        
        # æ–¹å¼ 1: é€’å½’æŸ¥æ‰¾
        try:
            pdf_files = list(Path(self.pdf_folder).glob("**/*.pdf"))
        except Exception as e:
            print(f"   è­¦å‘Šï¼šé€’å½’æŸ¥æ‰¾å¤±è´¥ ({e})ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾...")
        
        # æ–¹å¼ 2: ç›´æ¥åœ¨å½“å‰æ–‡ä»¶å¤¹æŸ¥æ‰¾
        if not pdf_files:
            try:
                pdf_files = list(Path(self.pdf_folder).glob("*.pdf"))
            except:
                pass
        
        # æ–¹å¼ 3: ä½¿ç”¨ os.listdir
        if not pdf_files:
            try:
                all_files = os.listdir(self.pdf_folder)
                pdf_files = [
                    Path(self.pdf_folder) / f 
                    for f in all_files 
                    if f.lower().endswith('.pdf')
                ]
            except Exception as e:
                print(f"   é”™è¯¯ï¼šæ— æ³•è®¿é—®ç›®å½• {self.pdf_folder}: {e}")
                return []
        
        if not pdf_files:
            print(f"   âš ï¸  åœ¨ {self.pdf_folder} ä¸­æœªæ‰¾åˆ° PDF æ–‡ä»¶")
            # æ‰“å°ç›®å½•å†…å®¹ç”¨äºè°ƒè¯•
            try:
                contents = os.listdir(self.pdf_folder)
                print(f"   ç›®å½•å†…å®¹ï¼š{contents[:10]}")  # æ˜¾ç¤ºå‰ 10 ä¸ªæ–‡ä»¶
            except:
                print(f"   æ— æ³•è¯»å–ç›®å½•å†…å®¹")
            return []
        
        print(f"   æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
        
        for pdf_file in pdf_files:
            try:
                print(f"   å¤„ç†: {pdf_file.name}...", end="", flush=True)
                doc = self._extract_pdf_content(pdf_file)
                if doc:
                    self.documents.append(doc)
                    print(" âœ“")
                else:
                    print(" âœ—ï¼ˆæ–‡ä»¶ä¸ºç©ºï¼‰")
            except Exception as e:
                print(f" âœ— ({str(e)[:50]})")
        
        print(f"   æ€»å…±åŠ è½½ {len(self.documents)} ç¯‡æ–‡ç« ")
        return self.documents
    
    def _extract_pdf_content(self, pdf_path: Path) -> Dict:
        """æå– PDF å†…å®¹å’Œå…ƒæ•°æ®"""
        reader = PdfReader(pdf_path)
        
        # æå–æ–‡æœ¬
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        
        # å°è¯•æå–å…ƒæ•°æ®
        metadata = reader.metadata or {}
        
        return {
            "filename": pdf_path.name,
            "text": text,
            "pages": len(reader.pages),
            "title": metadata.get("/Title", pdf_path.stem),
            "author": metadata.get("/Author", "æœªçŸ¥"),
            "created_date": metadata.get("/CreationDate", "æœªçŸ¥"),
            "keywords": metadata.get("/Keywords", ""),
        }

# åŠ è½½ PDF
pdf_loader = PDFArticleLoader(r"D:\HF_models\knowledge_base")
documents = pdf_loader.load_pdfs()

if not documents:
    print("\n   âœ— æ²¡æœ‰åŠ è½½åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼")
    print("   è¯·æ£€æŸ¥ D:\\knowledge_base æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰ PDF æ–‡ä»¶")
    exit(1)

# ============ Step 4: æ–‡æœ¬åˆ†å—ï¼ˆä¸ç”¨ langchainï¼‰ ============
print("\n4ï¸âƒ£  åˆ†å—å¤„ç†æ–‡æœ¬...")

class SimpleTextSplitter:
    """ç®€å•çš„æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size=800, chunk_overlap=150):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text: str) -> List[str]:
        """æŒ‰å­—ç¬¦é•¿åº¦åˆ†å—"""
        chunks = []
        overlap = self.chunk_overlap
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) < self.chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_documents(self, docs: List[Dict]) -> List[Dict]:
        """åˆ†å—å¹¶ä¿ç•™æ¥æºä¿¡æ¯"""
        chunks = []
        
        for doc in docs:
            text_chunks = self.split_text(doc["text"])
            
            for i, chunk in enumerate(text_chunks):
                if len(chunk.strip()) > 100:  # å¿½ç•¥å¤ªçŸ­çš„å—
                    chunks.append({
                        "content": chunk,
                        "source_file": doc["filename"],
                        "source_title": doc["title"],
                        "source_author": doc["author"],
                        "source_date": doc["created_date"],
                        "source_keywords": doc["keywords"],
                        "chunk_id": i,
                    })
        
        return chunks

splitter = SimpleTextSplitter(chunk_size=800, chunk_overlap=150)
chunks = splitter.split_documents(documents)
print(f"   åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")

# ============ Step 5: ç”Ÿæˆå‘é‡åµŒå…¥ ============
print("\n5ï¸âƒ£  ç”Ÿæˆå‘é‡åµŒå…¥...")

print("   åŠ è½½ embedding æ¨¡å‹...")
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("   âœ“ embedding æ¨¡å‹åŠ è½½æˆåŠŸ")

print(f"   ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡...")
texts_to_embed = [chunk["content"] for chunk in chunks]
embeddings = embedding_model.encode(texts_to_embed, show_progress_bar=True)
print(f"   âœ“ å‘é‡ç”Ÿæˆå®Œæˆ (ç»´åº¦: {embeddings.shape[1]})")

# ============ Step 6: æ„å»º FAISS ç´¢å¼• ============
print("\n6ï¸âƒ£  æ„å»º FAISS å‘é‡ç´¢å¼•...")

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings.astype(np.float32))
print(f"   âœ“ ç´¢å¼•æ„å»ºå®Œæˆ (åŒ…å« {index.ntotal} ä¸ªå‘é‡)")

# ============ Step 7: RAG æ£€ç´¢å’Œç”Ÿæˆ ============
print("\n7ï¸âƒ£  RAG ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
print("="*70)

def retrieve_and_generate(query: str, top_k: int = 1) -> Dict:
    """
    æ£€ç´¢ç›¸å…³æ–‡ç« å¹¶ç”Ÿæˆå›ç­”ï¼ˆæé€Ÿç‰ˆï¼‰
    """
    
    print(f"\nğŸ“ é—®é¢˜: {query}")
    print("-" * 70)
    
    # 1. å‘é‡åŒ–æŸ¥è¯¢
    query_embedding = embedding_model.encode([query])[0]
    
    # 2. æ£€ç´¢ç›¸å…³æ–‡æœ¬
    distances, indices = index.search(
        np.array([query_embedding]).astype(np.float32),
        min(top_k, len(chunks))
    )
    
    # 3. ç»„ç»‡ä¸Šä¸‹æ–‡ï¼ˆåªå–å‰ 500 å­—ç¬¦ï¼‰
    context = ""
    source_info = []
    
    for idx, distance in zip(indices[0], distances[0]):
        chunk = chunks[int(idx)]
        # åªå–å‰ 500 å­—ç¬¦ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
        truncated_content = chunk['content'][:500]
        context += f"ã€{chunk['source_title']}ã€‘\n{truncated_content}\n\n"
        
        source_key = chunk['source_file']
        if source_key not in [s['file'] for s in source_info]:
            source_info.append({
                "file": source_key,
                "title": chunk['source_title'],
                "author": chunk['source_author'],
            })
    
    # 4. æç®€æç¤ºè¯ï¼ˆå…³é”®ï¼åŠ å¿« 50%ï¼‰
    prompt = f"""å‚è€ƒï¼š{context}

Q: {query}
A:"""
    
    # 5. ç”¨ Qwen2.5-3B ç”Ÿæˆå›ç­”
    print("ğŸ¤– æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆé¢„è®¡ 30-60 ç§’ï¼‰...")
    start_time = time.time()
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,  # è¿›ä¸€æ­¥å‡å°ï¼ŒåŠ å¿« 2 å€
            do_sample=False,  # å…³é—­é‡‡æ ·ï¼ŒåŠ å¿«ç”Ÿæˆ
            pad_token_id=tokenizer.eos_token_id
        )
    
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–ç­”æ¡ˆéƒ¨åˆ†
    if "A:" in answer:
        answer = answer.split("A:")[-1].strip()
    
    elapsed = time.time() - start_time
    
    return {
        "query": query,
        "answer": answer,
        "sources": source_info,
        "time": elapsed
    }

# ============ Step 8: æµ‹è¯• ============
print("\n" + "="*70)
print("ğŸ§ª RAG ç³»ç»Ÿæµ‹è¯•")
print("="*70)

test_queries = [
    "è¿™äº›æ–‡ç« ä¸»è¦è®²ä»€ä¹ˆï¼Ÿ",
    "æ–‡ç« ä¸­æåˆ°çš„å…³é”®æ¦‚å¿µæœ‰å“ªäº›ï¼Ÿ",
    "æœ‰å“ªäº›åº”ç”¨åœºæ™¯è¢«æåˆ°ï¼Ÿ"
]

for i, query in enumerate(test_queries, 1):
    print(f"\n\n{'='*70}")
    print(f"æµ‹è¯• {i}/3")
    print('='*70)
    
    result = retrieve_and_generate(query, top_k=2)
    
    print(f"\nğŸ’¬ å›ç­”ï¼š")
    print(result["answer"])
    
    print(f"\nğŸ“š æ¥æºæ–‡ç« ï¼š")
    for source in result["sources"]:
        print(f"   - {source['title']} (ä½œè€…: {source['author']})")
    
    print(f"\nâ±ï¸  è€—æ—¶: {result['time']:.2f} ç§’")

print("\n\n" + "="*70)
print("âœ… RAG ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
print("="*70)
print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š")
print("   1. ä¿®æ”¹ test_queries åˆ—è¡¨æ¥æå‡ºä½ è‡ªå·±çš„é—®é¢˜")
print("   2. è°ƒæ•´ top_k å‚æ•°æ¥æ”¹å˜æ£€ç´¢çš„ç›¸å…³æ–‡æœ¬æ•°é‡")
print("   3. ä¿®æ”¹ chunk_size æ¥è°ƒæ•´åˆ†å—å¤§å°")