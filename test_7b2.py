"""
âœ… åŠ è½½æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹
- è¯†åˆ«å¹¶ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹
- ä¸ä¼šé‡æ–°ä¸‹è½½
- æ”¯æŒæœ¬åœ°è·¯å¾„ç›´æ¥åŠ è½½
"""

import torch
import time
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

def load_local_model():
    """åŠ è½½æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ“¦ åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆQwen2-7B GPTQï¼‰")
    print("="*70)
    
    # æ–¹æ³• 1ï¼šç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆæ¨èï¼ï¼‰
    # è¿™æ ·å¯ä»¥100%ç¡®ä¿åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œä¸ä¼šé‡æ–°ä¸‹è½½
    model_path = r"D:\HF_models\models--Qwen2.5-3B-Instruct-GPTQ-Int4\snapshots\main"
    
    print(f"\nğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼")
        return None, None
    
    print(f"âœ“ æ¨¡å‹è·¯å¾„å­˜åœ¨")
    
    # åˆ—å‡ºæ¨¡å‹æ–‡ä»¶
    print(f"\nğŸ“‹ æ¨¡å‹æ–‡ä»¶:")
    try:
        files = os.listdir(model_path)
        for f in sorted(files):
            full_path = os.path.join(model_path, f)
            if os.path.isfile(full_path):
                size_mb = os.path.getsize(full_path) / 1e6
                print(f"   - {f} ({size_mb:.1f}MB)")
            else:
                print(f"   - {f}/ (æ–‡ä»¶å¤¹)")
    except Exception as e:
        print(f"   âœ— æ— æ³•åˆ—å‡ºæ–‡ä»¶: {e}")
    
    print(f"\n1ï¸âƒ£  åŠ è½½ tokenizerï¼ˆä»æœ¬åœ°è·¯å¾„ï¼‰...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,  # â† ç›´æ¥ç”¨æœ¬åœ°è·¯å¾„ï¼Œä¸ç”¨æ¨¡å‹ ID
            trust_remote_code=True,
            local_files_only=True  # â† å…³é”®ï¼åªä»æœ¬åœ°åŠ è½½
        )
        print("   âœ“ tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âœ— tokenizer åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    print(f"\n2ï¸âƒ£  åŠ è½½æ¨¡å‹ï¼ˆä»æœ¬åœ°è·¯å¾„ï¼‰...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,  # â† ç›´æ¥ç”¨æœ¬åœ°è·¯å¾„ï¼Œä¸ç”¨æ¨¡å‹ ID
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True  # â† å…³é”®ï¼åªä»æœ¬åœ°åŠ è½½
        )
        print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    model.eval()
    
    return model, tokenizer


def generate_text(model, tokenizer, prompt, max_tokens=100):
    """ç”Ÿæˆæ–‡æœ¬"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.8,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    elapsed = time.time() - start_time
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    tokens_generated = len(outputs[0]) - len(inputs["input_ids"][0])
    
    return result, elapsed, tokens_generated


def main():
    """ä¸»ç¨‹åº"""
    
    print("\n" + "ğŸš€ " + "="*66 + " ğŸš€")
    print("    Qwen2-7B GPTQ æœ¬åœ°æ¨¡å‹")
    print("    åŠ è½½æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼ˆä¸ä¼šé‡æ–°ä¸‹è½½ï¼‰")
    print("ğŸš€ " + "="*66 + " ğŸš€")
    
    try:
        # åŠ è½½æœ¬åœ°æ¨¡å‹
        model, tokenizer = load_local_model()
        
        if model is None or tokenizer is None:
            print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\n" + "="*70)
        print("â„¹ï¸  æ¨¡å‹ä¿¡æ¯")
        print("="*70)
        print(f"æ¨¡å‹: Qwen2-7B GPTQ 4bit")
        print(f"è®¾å¤‡: {next(model.parameters()).device}")
        print(f"æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
        
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"GPU æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
        
        # æµ‹è¯•ç”Ÿæˆ
        print("\n" + "="*70)
        print("ğŸ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
        print("="*70)
        
        test_prompts = [
            "AIçš„æœªæ¥æ˜¯",
            "æœºå™¨å­¦ä¹ çš„ä¸‰ä¸ªä¸»è¦æ–¹å‘åŒ…æ‹¬",
            "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨",
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}ï¸âƒ£  æç¤ºè¯: '{prompt}'")
            print("-" * 70)
            
            result, elapsed, tokens = generate_text(
                model, tokenizer, prompt, max_tokens=80
            )
            
            print(f"ç”Ÿæˆç»“æœ:")
            print(result)
            print("-" * 70)
            print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}s | ğŸ“Š é€Ÿåº¦: {tokens/elapsed:.2f} tokens/s")
        
        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*70)
        print("\nğŸ’¡ ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„ä¼˜ç‚¹:")
        print("   - ä¸éœ€è¦ç½‘ç»œè¿æ¥")
        print("   - åŠ è½½é€Ÿåº¦å¿«")
        print("   - ä¸ä¼šé‡å¤ä¸‹è½½")
        print("="*70 + "\n")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()