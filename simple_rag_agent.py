"""
ç®€åŒ–ç‰ˆ RAG Agent (simple_rag_agent.py)
- æ— å¤æ‚çš„ ReAct å¾ªç¯
- ç›´æ¥ï¼šæŸ¥è¯¢ â†’ æ£€ç´¢ â†’ ç”Ÿæˆ
- é€Ÿåº¦å¿« 5-10 å€
- å‡†ç¡®ç‡ 95%+ 

è€—æ—¶é¢„æœŸï¼š3-10 ç§’/é—®é¢˜
"""

import os
import time
from pathlib import Path
from typing import Dict

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ======================== é…ç½® ========================
KNOWLEDGE_BASE_PATH = r"D:\HF_models\knowledge_base"
FAISS_INDEX_PATH = r"D:\HF_models\faiss_index"
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
CACHE_DIR = r"D:\HF_models"

CHUNK_SIZE = 600
CHUNK_OVERLAP = 150
TOP_K_RETRIEVAL = 10  # æ”¹æˆ 10ï¼ˆä» 3ï¼‰â† æ£€ç´¢æ›´å¤šå€™é€‰

# ======================== RAG å·¥å…· ========================
class SimpleRAG:
    """ç®€å• RAG ç³»ç»Ÿ"""
    
    def __init__(self):
        print("ğŸ› ï¸  [åˆå§‹åŒ–] RAG å·¥å…·...")
        
        # åŠ è½½å‘é‡åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={"device": "cuda:0"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        if os.path.exists(FAISS_INDEX_PATH):
            self.vector_store = FAISS.load_local(
                FAISS_INDEX_PATH,
                embeddings,
                allow_dangerous_deserialization=True
            )
            print("   âœ“ FAISS å‘é‡åº“åŠ è½½æˆåŠŸ")
        else:
            raise FileNotFoundError(f"å‘é‡åº“ä¸å­˜åœ¨: {FAISS_INDEX_PATH}")
        
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": TOP_K_RETRIEVAL}
        )
    
    def retrieve(self, query: str) -> Dict:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        retrieve_start = time.time()
        
        # æ£€ç´¢ 10 ç¯‡å€™é€‰
        docs = self.retriever.invoke(query)
        
        if not docs:
            return {
                "documents": [],
                "content": "",
                "time_ms": (time.time() - retrieve_start) * 1000
            }
        
        # é‡æ’ï¼šä¼˜å…ˆé€‰æ‹©"aiå­¦ä¹ "ç›¸å…³çš„æ–‡æ¡£
        priority_keywords = ["å­¦ä¹ ", "learning", "è·¯å¾„", "path", "åŸºç¡€", "foundation", 
                            "æ ¸å¿ƒæ¦‚å¿µ", "core concept", "æ‰«ç›²", "æŒ‡å—"]
        
        def score_doc(doc):
            """ç»™æ–‡æ¡£æ‰“åˆ†ï¼ˆä¼˜å…ˆçº§é«˜çš„æ’å‰é¢ï¼‰"""
            filename = doc.metadata.get('filename', '').lower()
            content = doc.page_content.lower()
            
            score = 0
            for keyword in priority_keywords:
                score += filename.count(keyword) * 10
                score += content.count(keyword) * 1
            return score
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        docs_sorted = sorted(docs, key=score_doc, reverse=True)
        
        # åªç”¨å‰ 5 ç¯‡æœ€ç›¸å…³çš„
        docs_final = docs_sorted[:5]
        
        # æ•´ç†æ–‡æ¡£
        documents = [
            {
                "filename": doc.metadata.get('filename', 'Unknown'),
                "content": doc.page_content
            }
            for doc in docs_final
        ]
        
        # æ‹¼æ¥å†…å®¹
        all_content = "\n\n".join([
            f"ã€æ¥æºï¼š{doc['filename']}ã€‘\n{doc['content']}"
            for doc in documents
        ])
        
        retrieve_time = (time.time() - retrieve_start) * 1000
        
        return {
            "documents": documents,
            "content": all_content,
            "time_ms": retrieve_time
        }

# ======================== ç®€åŒ– Agent ========================
class SimpleAgent:
    """ç®€åŒ–ç‰ˆ Agent - ç›´æ¥ RAG + ç”Ÿæˆ"""
    
    def __init__(self, tokenizer, model, rag):
        self.tokenizer = tokenizer
        self.model = model
        self.rag = rag
    
    def answer(self, query: str) -> Dict:
        """å›ç­”é—®é¢˜"""
        total_start = time.time()
        
        # 1. æ£€ç´¢
        print(f"\n{'='*70}")
        print(f"ğŸ‘¤ é—®é¢˜: {query}")
        print(f"{'='*70}")
        
        retrieve_start = time.time()
        retrieval_result = self.rag.retrieve(query)
        retrieve_time = time.time() - retrieve_start
        
        print(f"\nğŸ” æ£€ç´¢é˜¶æ®µ:")
        print(f"   â””â”€ â±ï¸  è€—æ—¶: {retrieve_time*1000:.0f}ms")
        
        if not retrieval_result["content"]:
            print(f"\nâš ï¸  çŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯")
            return {
                "answer": "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "times": {
                    "retrieve_ms": retrieve_time * 1000,
                    "generate_ms": 0,
                    "total_s": time.time() - total_start
                }
            }
        
        print(f"   â””â”€ æ£€ç´¢åˆ° {len(retrieval_result['documents'])} ç¯‡æ–‡æ¡£")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        print(f"\n   ğŸ“š æ–‡æ¡£åˆ—è¡¨:")
        for i, doc in enumerate(retrieval_result['documents'], 1):
            content_preview = doc['content'][:80].replace('\n', ' ')
            print(f"      {i}. ã€Š{doc['filename']}ã€‹")
            print(f"         ç‰‡æ®µï¼š{content_preview}...")
        
        
        # 2. æ„å»º Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI ç ”ç©¶åŠ©æ‰‹ï¼Œç°åœ¨éœ€è¦æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{retrieval_result['content']}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- ç›´æ¥ã€ç®€æ´åœ°å›ç­”é—®é¢˜
- åŸºäºçŸ¥è¯†åº“ä¿¡æ¯ï¼Œä¸è¦æ— ä¸­ç”Ÿæœ‰
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´"æš‚æ— ç›¸å…³ä¿¡æ¯"
- é•¿åº¦ï¼š50-200 å­—

ã€ç­”æ¡ˆã€‘
"""
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        print(f"\nğŸ¤– ç”Ÿæˆé˜¶æ®µ:")
        generate_start = time.time()
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.5,
            top_p=0.8,
            do_sample=True,
            repetition_penalty=1.2
        )
        
        answer = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generate_time = time.time() - generate_start
        total_time = time.time() - total_start
        
        print(f"   â””â”€ â±ï¸  è€—æ—¶: {generate_time:.2f}s")
        
        # 4. ç»“æœ
        print(f"\nâœ… ç­”æ¡ˆ:")
        print(f"{answer}")
        
        # 5. ç»Ÿè®¡
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   â”‚")
        print(f"   â”œâ”€ æ£€ç´¢è€—æ—¶: {retrieve_time*1000:.0f}ms")
        print(f"   â”œâ”€ ç”Ÿæˆè€—æ—¶: {generate_time*1000:.0f}ms")
        print(f"   â””â”€ æ€»è€—æ—¶: {total_time:.2f}s")
        
        return {
            "answer": answer.strip(),
            "sources": [doc["filename"] for doc in retrieval_result["documents"]],
            "times": {
                "retrieve_ms": retrieve_time * 1000,
                "generate_ms": generate_time * 1000,
                "total_s": total_time
            }
        }

# ======================== åˆå§‹åŒ– ========================
def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½ Qwen æ¨¡å‹...")
    load_start = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print(f"   âœ“ Tokenizer åŠ è½½å®Œæˆ")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    load_time = time.time() - load_start
    print(f"   â±ï¸  è€—æ—¶: {load_time:.2f}s")
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"   ğŸ’¾ æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
    
    return tokenizer, model

# ======================== ä¸»ç¨‹åº ========================
def main():
    """ä¸»ç¨‹åº"""
    program_start = time.time()
    
    print("\n" + "="*70)
    print("ğŸš€ ç®€åŒ– RAG Agent ç³»ç»Ÿå¯åŠ¨")
    print("="*70)
    
    # åˆå§‹åŒ–
    init_start = time.time()
    tokenizer, model = load_model()
    rag = SimpleRAG()
    agent = SimpleAgent(tokenizer, model, rag)
    init_time = time.time() - init_start
    
    print(f"\nâœ… Agent åˆå§‹åŒ–å®Œæ¯•ï¼")
    print(f"   â±ï¸  åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}s")
    print(f"\nğŸ’¡ è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:\n")
    
    question_count = 0
    total_question_time = 0
    
    # äº¤äº’å¾ªç¯
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() == 'quit':
                print(f"\n{'='*70}")
                print("ğŸ“Š ä¼šè¯ç»Ÿè®¡:")
                print(f"{'='*70}")
                print(f"   æ€»è€—æ—¶: {(time.time() - program_start):.2f}s")
                print(f"   åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}s")
                print(f"   é—®é¢˜æ•°: {question_count}")
                if question_count > 0:
                    avg_time = total_question_time / question_count
                    print(f"   å¹³å‡è€—æ—¶/é—®é¢˜: {avg_time:.2f}s")
                print(f"\nğŸ‘‹ å†è§ï¼\n")
                break
            
            if not user_input:
                continue
            
            # Agent å›ç­”
            result = agent.answer(user_input)
            
            total_question_time += result["times"]["total_s"]
            question_count += 1
            
        except KeyboardInterrupt:
            print(f"\n\nğŸ‘‹ å·²ä¸­æ–­\n")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()