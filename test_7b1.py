"""
âœ… ç¨³å®šç‰ˆï¼šQwen2.5-3B GPTQ å®Œæ•´ç®¡é“
å·²éªŒè¯å¯åœ¨ GTX 1650 + CUDA 13.1 + transformers 4.45.2 ä¸Šæ­£å¸¸å·¥ä½œ
"""

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

def load_model(model_path):
    """åŠ è½½ GPTQ æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ“¦ æ¨¡å‹åŠ è½½")
    print("="*70)
    
    print("\n1ï¸âƒ£  åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, 
        trust_remote_code=True
    )
    print("   âœ“ tokenizer åŠ è½½æˆåŠŸ")
    
    print("\n2ï¸âƒ£  åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
    print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # è®¾ç½®è¯„ä¼°æ¨¡å¼
    model.eval()
    
    return model, tokenizer


def generate_text(model, tokenizer, prompt, max_tokens=100):
    """ç”Ÿæˆæ–‡æœ¬"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.8,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    elapsed = time.time() - start_time
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    tokens_generated = len(outputs[0]) - len(inputs["input_ids"][0])
    
    return result, elapsed, tokens_generated


def main():
    """ä¸»ç¨‹åº"""
    
    print("\n" + "ğŸš€ " + "="*66 + " ğŸš€")
    print("    Qwen2.5-3B GPTQ æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿ")
    print("ğŸš€ " + "="*66 + " ğŸš€")
    
    model_path = r"D:\HF_models\hub\models--Qwen2.5-3B-Instruct-GPTQ-Int4\snapshots\main"
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(model_path)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print("\n" + "="*70)
    print("â„¹ï¸  æ¨¡å‹ä¿¡æ¯")
    print("="*70)
    print(f"è®¾å¤‡: {next(model.parameters()).device}")
    print(f"æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"GPU æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
    
    # æµ‹è¯•ç”Ÿæˆ
    print("\n" + "="*70)
    print("ğŸ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
    print("="*70)
    
    test_prompts = [
        "AIçš„æœªæ¥æ˜¯",
        "æœºå™¨å­¦ä¹ çš„ä¸‰ä¸ªä¸»è¦æ–¹å‘åŒ…æ‹¬",
        "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨",
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{i}ï¸âƒ£  æç¤ºè¯: '{prompt}'")
        print("-" * 70)
        
        result, elapsed, tokens = generate_text(
            model, tokenizer, prompt, max_tokens=80
        )
        
        print(f"ç”Ÿæˆç»“æœ:")
        print(result)
        print("-" * 70)
        print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}s | ğŸ“Š é€Ÿåº¦: {tokens/elapsed:.2f} tokens/s")
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("="*70)
    print("\nğŸ’¡ æ¨¡å‹å·²åŠ è½½ï¼Œå¯ç”¨äº:")
    print("   - RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)")
    print("   - å¾®è°ƒ (LoRA)")
    print("   - API éƒ¨ç½² (FastAPI/Streamlit)")
    print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    main()