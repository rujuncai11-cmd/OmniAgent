"""
âœ… Qwen2.5-3B FP16 å®˜æ–¹ç‰ˆæœ¬ - æ˜¾ç¤ºæ¨¡å‹è·¯å¾„
- è‡ªåŠ¨ä¸‹è½½å®˜æ–¹æ¨¡å‹
- æ˜¾ç¤ºæ¨¡å‹æ‰€åœ¨çš„å®é™…è·¯å¾„
- æ˜¾å­˜å ç”¨ï¼š4GB
- å®Œå…¨æ— ä¹±ç 
"""

import torch
import time
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import snapshot_download

def find_model_path(model_id, cache_dir):
    """æŸ¥æ‰¾æ¨¡å‹çš„å®é™…è·¯å¾„"""
    print(f"\nğŸ” æŸ¥æ‰¾æ¨¡å‹è·¯å¾„...")
    
    # è·å– HF é»˜è®¤ç¼“å­˜ä½ç½®
    from huggingface_hub import HfApi
    api = HfApi()
    
    # æ¨¡å‹ ID è½¬æ¢ä¸ºæ–‡ä»¶å¤¹å
    # "Qwen/Qwen2.5-3B-Instruct" -> "models--Qwen--Qwen2.5-3B-Instruct"
    repo_name = model_id.replace("/", "--")
    model_folder_name = f"models--{repo_name}"
    
    # æ£€æŸ¥å¯èƒ½çš„è·¯å¾„
    possible_paths = [
        os.path.join(cache_dir, "hub", model_folder_name),  # æŒ‡å®šçš„ cache_dir
        os.path.join(cache_dir, model_folder_name),          # cache_dir ç›´æ¥ä¸‹
        os.path.expanduser(f"~/.cache/huggingface/hub/{model_folder_name}"),  # é»˜è®¤ä½ç½®
    ]
    
    print(f"   æ£€æŸ¥ä»¥ä¸‹è·¯å¾„:")
    for path in possible_paths:
        print(f"   - {path}")
        if os.path.exists(path):
            print(f"     âœ“ æ‰¾åˆ°!")
            return path
    
    print(f"   âœ— æœªåœ¨ä¸Šè¿°ä½ç½®æ‰¾åˆ°")
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸‹è½½/æ£€æŸ¥
    try:
        print(f"\n   å°è¯•ä½¿ç”¨ snapshot_download æŸ¥è¯¢...")
        actual_path = snapshot_download(
            model_id,
            cache_dir=cache_dir,
            resume_download=True
        )
        print(f"   âœ“ æ¨¡å‹è·¯å¾„: {actual_path}")
        return actual_path
    except Exception as e:
        print(f"   âœ— æŸ¥è¯¢å¤±è´¥: {e}")
        return None


def load_model():
    """åŠ è½½ FP16 å®˜æ–¹ç‰ˆæœ¬"""
    print("\n" + "="*70)
    print("ğŸ“¦ æ¨¡å‹åŠ è½½ï¼ˆFP16 å®˜æ–¹ç‰ˆæœ¬ï¼‰")
    print("="*70)
    
    # ä½¿ç”¨å®˜æ–¹æœªé‡åŒ–çš„ FP16 ç‰ˆæœ¬
    model_id = "Qwen/Qwen2.5-3B-Instruct"
    cache_dir = r"D:\HF_models"
    
    print(f"\nğŸ“ é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹ ID: {model_id}")
    print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")
    
    # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
    model_path = find_model_path(model_id, cache_dir)
    
    print(f"\n1ï¸âƒ£  åŠ è½½ tokenizer...")
    print(f"   ï¼ˆé¦–æ¬¡ä¼šä» HuggingFace ä¸‹è½½ï¼Œçº¦ 1GBï¼‰")
    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    print("   âœ“ tokenizer åŠ è½½æˆåŠŸ")
    
    print(f"\n2ï¸âƒ£  åŠ è½½æ¨¡å‹...")
    print(f"   ï¼ˆé¦–æ¬¡ä¼šä» HuggingFace ä¸‹è½½ï¼Œçº¦ 6GBï¼‰")
    print(f"   è¯·è€å¿ƒç­‰å¾…...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # å†æ¬¡æŸ¥è¯¢ï¼ˆç¡®ä¿æ‰¾åˆ°ï¼‰
    model_path = find_model_path(model_id, cache_dir)
    
    model.eval()
    
    return model, tokenizer, model_path


def generate_text(model, tokenizer, prompt, max_tokens=100):
    """ç”Ÿæˆæ–‡æœ¬"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.8,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    elapsed = time.time() - start_time
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    tokens_generated = len(outputs[0]) - len(inputs["input_ids"][0])
    
    return result, elapsed, tokens_generated


def main():
    """ä¸»ç¨‹åº"""
    
    print("\n" + "ğŸš€ " + "="*66 + " ğŸš€")
    print("    Qwen2.5-3B FP16 å®˜æ–¹ç‰ˆæœ¬")
    print("    è‡ªåŠ¨ä¸‹è½½ + æ˜¾ç¤ºè·¯å¾„ + ç”Ÿæˆæµ‹è¯•")
    print("ğŸš€ " + "="*66 + " ğŸš€")
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
        model, tokenizer, model_path = load_model()
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\n" + "="*70)
        print("â„¹ï¸  æ¨¡å‹ä¿¡æ¯")
        print("="*70)
        print(f"ç‰ˆæœ¬: FP16 å®˜æ–¹ç‰ˆæœ¬ï¼ˆæœªé‡åŒ–ï¼‰")
        print(f"è®¾å¤‡: {next(model.parameters()).device}")
        print(f"æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
        
        if model_path:
            print(f"\nğŸ“‚ æ¨¡å‹è·¯å¾„:")
            print(f"   {model_path}")
            
            # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
            if os.path.exists(model_path):
                print(f"\nğŸ“‹ æ¨¡å‹æ–‡ä»¶:")
                try:
                    files = os.listdir(model_path)
                    for f in sorted(files)[:10]:  # åªæ˜¾ç¤ºå‰ 10 ä¸ªæ–‡ä»¶
                        full_path = os.path.join(model_path, f)
                        if os.path.isfile(full_path):
                            size_mb = os.path.getsize(full_path) / 1e6
                            print(f"   - {f} ({size_mb:.1f}MB)")
                        else:
                            print(f"   - {f}/ (æ–‡ä»¶å¤¹)")
                    if len(files) > 10:
                        print(f"   ... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
                except Exception as e:
                    print(f"   æ— æ³•åˆ—å‡ºæ–‡ä»¶: {e}")
        
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"\nğŸ’¾ GPU æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
        
        # æµ‹è¯•ç”Ÿæˆ
        print("\n" + "="*70)
        print("ğŸ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
        print("="*70)
        
        test_prompts = [
            "AIçš„æœªæ¥æ˜¯",
            "æœºå™¨å­¦ä¹ çš„ä¸‰ä¸ªä¸»è¦æ–¹å‘åŒ…æ‹¬",
            "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„åº”ç”¨",
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}ï¸âƒ£  æç¤ºè¯: '{prompt}'")
            print("-" * 70)
            
            result, elapsed, tokens = generate_text(
                model, tokenizer, prompt, max_tokens=80
            )
            
            print(f"ç”Ÿæˆç»“æœ:")
            print(result)
            print("-" * 70)
            print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}s | ğŸ“Š é€Ÿåº¦: {tokens/elapsed:.2f} tokens/s")
        
        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*70)
        print("\nğŸ’¡ æ¨¡å‹å·²åŠ è½½ï¼Œå¯ç”¨äº:")
        print("   - RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)")
        print("   - å¾®è°ƒ (LoRA)")
        print("   - API éƒ¨ç½² (FastAPI/Streamlit)")
        print("\nä¸‹æ¬¡è¿è¡Œä¼šæ›´å¿«ï¼ˆä¸éœ€è¦é‡æ–°ä¸‹è½½ï¼‰")
        print("="*70 + "\n")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œå°è¯•ï¼š")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. ç¡®ä¿èƒ½è®¿é—® huggingface.co")
        print("   3. æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆéœ€è¦ 15GB ä»¥ä¸Šï¼‰")


if __name__ == "__main__":
    main()