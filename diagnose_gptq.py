"""
GPTQ ä¿®å¤éªŒè¯è„šæœ¬
é€æ­¥è¯Šæ–­å’Œä¿®å¤ GPTQ åŠ è½½é—®é¢˜
"""

import os
import sys
import subprocess

def check_versions():
    """æ£€æŸ¥å…³é”®åº“ç‰ˆæœ¬"""
    print("\n" + "="*70)
    print("1ï¸âƒ£  ç‰ˆæœ¬æ£€æŸ¥")
    print("="*70)
    
    packages = {
        'torch': 'éœ€è¦ 2.0+',
        'transformers': 'éœ€è¦ 4.45.2ï¼ˆæˆ– 4.45.xï¼‰',
        'auto-gptq': 'éœ€è¦ 0.7.1+',
        'optimum': 'éœ€è¦ 1.17.0+',
    }
    
    import torch
    from transformers import __version__ as transformers_version
    
    print(f"\nâœ“ torch: {torch.__version__}")
    print(f"  å»ºè®®: 2.0+ï¼Œä½ çš„å¯ä»¥")
    
    print(f"\nâœ“ transformers: {transformers_version}")
    if transformers_version.startswith(('4.45', '4.46', '4.47', '4.48')):
        print(f"  âœ… ç‰ˆæœ¬ OKï¼ˆç¨³å®šç‰ˆæœ¬ï¼‰")
    elif transformers_version.startswith('4.57'):
        print(f"  âš ï¸  ç‰ˆæœ¬è¿‡æ–°ï¼ˆå·²çŸ¥æœ‰ GPTQ bugï¼‰")
        print(f"  ğŸ’¡ å»ºè®®é™çº§åˆ° 4.45.2")
        print(f"\n  è¿è¡Œè¿™æ¡å‘½ä»¤é™çº§ï¼š")
        print(f"  pip install transformers==4.45.2")
    else:
        print(f"  âš ï¸  ç‰ˆæœ¬æœªæµ‹è¯•")
    
    try:
        import auto_gptq
        print(f"\nâœ“ auto-gptq: {auto_gptq.__version__}")
    except ImportError:
        print(f"\nâŒ auto-gptq: æœªå®‰è£…")
    
    try:
        import optimum
        print(f"âœ“ optimum: {optimum.__version__}")
    except ImportError:
        print(f"âŒ optimum: æœªå®‰è£…")


def check_env_vars():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    print("\n" + "="*70)
    print("2ï¸âƒ£  ç¯å¢ƒå˜é‡æ£€æŸ¥")
    print("="*70)
    
    env_vars = {
        'DISABLE_EXLLAMA': 'åº”è¯¥æ˜¯ 1',
        'DISABLE_EXLLAMAV2': 'åº”è¯¥æ˜¯ 1',
        'EXLLAMA_NO_CUDA_EXTENSION': 'åº”è¯¥æ˜¯ 1',
    }
    
    for var_name, expected in env_vars.items():
        value = os.environ.get(var_name, 'æœªè®¾ç½®')
        status = "âœ“" if value == '1' else "âš ï¸"
        print(f"{status} {var_name}: {value}")
    
    if os.environ.get('DISABLE_EXLLAMA') != '1':
        print(f"\nğŸ’¡ ç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®")
        print(f"  è¿è¡Œè¿™æ¡å‘½ä»¤ï¼ˆéœ€è¦ç®¡ç†å‘˜ï¼‰ï¼š")
        print(f"  conda env config vars set DISABLE_EXLLAMA=1")
        print(f"  conda env config vars set DISABLE_EXLLAMAV2=1")
        print(f"  ç„¶åé‡æ–°æ¿€æ´»ç¯å¢ƒ")


def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print("\n" + "="*70)
    print("3ï¸âƒ£  æ¨¡å‹æ–‡ä»¶æ£€æŸ¥")
    print("="*70)
    
    model_path = r"D:\HF_models\hub\models--Qwen2.5-3B-Instruct-GPTQ-Int4\snapshots\main"
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    print(f"âœ“ æ¨¡å‹ç›®å½•å­˜åœ¨")
    
    files = os.listdir(model_path)
    required = ['config.json', 'tokenizer.json']
    weights = [f for f in files if f.endswith(('.safetensors', '.bin'))]
    
    print(f"\nä¸»è¦æ–‡ä»¶ï¼š")
    for req in required:
        if req in files:
            size = os.path.getsize(os.path.join(model_path, req)) / 1024
            print(f"  âœ“ {req} ({size:.0f}KB)")
        else:
            print(f"  âŒ {req} (ç¼ºå¤±)")
    
    if weights:
        for w in weights:
            size = os.path.getsize(os.path.join(model_path, w)) / (1024**3)
            print(f"  âœ“ {w} ({size:.1f}GB)")
    else:
        print(f"  âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ç¼ºå¤±ï¼")
        return False
    
    return True


def test_gptq_load():
    """æµ‹è¯• GPTQ åŠ è½½"""
    print("\n" + "="*70)
    print("4ï¸âƒ£  GPTQ åŠ è½½æµ‹è¯•")
    print("="*70)
    
    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        model_path = r"D:\HF_models\hub\models--Qwen2.5-3B-Instruct-GPTQ-Int4\snapshots\main"
        
        print(f"\nåŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print(f"âœ“ tokenizer åŠ è½½æˆåŠŸ")
        
        print(f"\nåŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # ç®€å•ç”Ÿæˆæµ‹è¯•
        print(f"\nç”Ÿæˆæµ‹è¯•...")
        prompt = "AIçš„æœªæ¥æ˜¯"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=30)
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ“ ç”ŸæˆæˆåŠŸï¼")
        print(f"\nç»“æœ: {result}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)[:200]}")
        return False


def suggest_fix():
    """å»ºè®®ä¿®å¤"""
    print("\n" + "="*70)
    print("ğŸ’¡ ä¿®å¤å»ºè®®")
    print("="*70)
    
    print("""
æ ¹æ®ä¸Šé¢çš„è¯Šæ–­ï¼Œè¿™æ˜¯ä¿®å¤æ­¥éª¤ï¼š

ã€å¦‚æœ transformers ç‰ˆæœ¬æ˜¯ 4.57.xã€‘
â†’ æœ€å¯èƒ½çš„åŸå› å°±æ˜¯ç‰ˆæœ¬ bug
â†’ è¿è¡Œè¿™æ¡å‘½ä»¤é™çº§ï¼š
   pip install transformers==4.45.2

ã€å¦‚æœç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®ã€‘
â†’ åœ¨ Anaconda Prompt ä¸­è¿è¡Œï¼š
   conda activate omniagent
   conda env config vars set DISABLE_EXLLAMA=1
   conda env config vars set DISABLE_EXLLAMAV2=1
   conda deactivate
   conda activate omniagent

ã€å¦‚æœæ¨¡å‹æ–‡ä»¶ç¼ºå¤±ã€‘
â†’ é‡æ–°ä¸‹è½½ï¼š
   huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4 \\
     --local-dir D:\\HF_models\\hub\\models--Qwen2.5-3B-Instruct-GPTQ-Int4

ã€å®Œæˆåï¼Œé‡æ–°è¿è¡Œè¿™ä¸ªè¯Šæ–­è„šæœ¬éªŒè¯ã€‘
    """)


def main():
    print("\nğŸ” GPTQ è¯Šæ–­ & ä¿®å¤å·¥å…·")
    print("="*70)
    
    check_versions()
    check_env_vars()
    files_ok = check_model_files()
    
    if not files_ok:
        print("\nâŒ æ¨¡å‹æ–‡ä»¶æœ‰é—®é¢˜ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        suggest_fix()
        return
    
    success = test_gptq_load()
    
    if success:
        print("\n" + "="*70)
        print("âœ… GPTQ å·¥ä½œæ­£å¸¸ï¼å¯ä»¥ç»§ç»­å¼€å‘")
        print("="*70)
    else:
        print("\n" + "="*70)
        print("âŒ GPTQ åŠ è½½å¤±è´¥")
        print("="*70)
        suggest_fix()


if __name__ == "__main__":
    main()