"""
ReAct Agent å®ç° (agent_v1.py)
å®Œæ•´çš„æ€è€ƒ->è¡ŒåŠ¨->è§‚å¯Ÿ->ç»“è®ºå¾ªç¯

åŠŸèƒ½ï¼š
1. æ€è€ƒï¼ˆThoughtï¼‰ï¼šåˆ†æé—®é¢˜éœ€è¦ä»€ä¹ˆä¿¡æ¯
2. è¡ŒåŠ¨ï¼ˆActionï¼‰ï¼šè°ƒç”¨ RAG å·¥å…·æ£€ç´¢çŸ¥è¯†åº“
3. è§‚å¯Ÿï¼ˆObservationï¼‰ï¼šåˆ†ææ£€ç´¢ç»“æœ
4. æœ€ç»ˆç­”æ¡ˆï¼ˆFinal Answerï¼‰ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Tuple
import re
import time
from datetime import datetime

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ======================== é…ç½® ========================
KNOWLEDGE_BASE_PATH = r"D:\HF_models\knowledge_base"
FAISS_INDEX_PATH = r"D:\HF_models\faiss_index"
MODEL_ID = "Qwen/Qwen2.5-3B-Instruct"
CACHE_DIR = r"D:\HF_models"

CHUNK_SIZE = 600
CHUNK_OVERLAP = 150
TOP_K_RETRIEVAL = 3

# ======================== å·¥å…·1ï¼šRAG æ£€ç´¢å·¥å…· ========================
class RAGTool:
    """RAG å·¥å…·ï¼šä»çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯"""
    
    def __init__(self):
        print("ğŸ› ï¸  [åˆå§‹åŒ–] RAG å·¥å…·...")
        
        # åŠ è½½å‘é‡åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={"device": "cuda:0"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        if os.path.exists(FAISS_INDEX_PATH):
            self.vector_store = FAISS.load_local(
                FAISS_INDEX_PATH,
                embeddings,
                allow_dangerous_deserialization=True
            )
            print("   âœ“ FAISS å‘é‡åº“åŠ è½½æˆåŠŸ")
        else:
            raise FileNotFoundError(f"å‘é‡åº“ä¸å­˜åœ¨: {FAISS_INDEX_PATH}")
        
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": TOP_K_RETRIEVAL}
        )
    
    def execute(self, query: str) -> Dict:
        """æ‰§è¡Œæ£€ç´¢"""
        start_time = time.time()
        
        docs = self.retriever.invoke(query)
        
        if not docs:
            elapsed = time.time() - start_time
            return {
                "success": False,
                "documents": [],
                "content": "çŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯",
                "time_ms": elapsed * 1000
            }
        
        # æ•´ç†æ£€ç´¢ç»“æœ
        documents = [
            {
                "filename": doc.metadata.get('filename', 'Unknown'),
                "content": doc.page_content[:300],  # å‰ 300 å­—
                "full_content": doc.page_content
            }
            for doc in docs
        ]
        
        # æ‹¼æ¥æ‰€æœ‰æ–‡æ¡£å†…å®¹
        all_content = "\n\n".join([
            f"ã€{doc['filename']}ã€‘\n{doc['full_content']}"
            for doc in documents
        ])
        
        elapsed = time.time() - start_time
        
        return {
            "success": True,
            "documents": documents,
            "content": all_content,
            "time_ms": elapsed * 1000
        }

# ======================== å·¥å…·2ï¼šè®¡ç®—å™¨å·¥å…· ========================
class CalculatorTool:
    """ç®€å•è®¡ç®—å™¨å·¥å…·"""
    
    def execute(self, expression: str) -> Dict:
        """æ‰§è¡Œè®¡ç®—"""
        try:
            result = eval(expression)
            return {
                "success": True,
                "result": str(result)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

# ======================== ReAct Agent ========================
class ReActAgent:
    """ReAct Agent å®ç°"""
    
    def __init__(self, tokenizer, model):
        """åˆå§‹åŒ– Agent"""
        self.tokenizer = tokenizer
        self.model = model
        
        # åˆå§‹åŒ–å·¥å…·
        self.tools = {
            "rag": RAGTool(),
            "calculator": CalculatorTool()
        }
        
        self.max_iterations = 5  # æœ€å¤šè¿­ä»£æ¬¡æ•°
        self.conversation_history = []  # å¯¹è¯å†å²
    
    def parse_action(self, text: str) -> Tuple[str, str]:
        """è§£æ Action å’Œ Input"""
        # åŒ¹é… Action: xxx å’Œ Action Input: yyy
        action_match = re.search(r"Action:\s*(\w+)", text)
        input_match = re.search(r"Action Input:\s*(.+?)(?:\n|$)", text)
        
        if action_match and input_match:
            action = action_match.group(1).strip()
            action_input = input_match.group(1).strip()
            return action, action_input
        
        return None, None
    
    def execute_tool(self, tool_name: str, tool_input: str) -> Tuple[str, float]:
        """æ‰§è¡Œå·¥å…·ï¼Œè¿”å›ç»“æœå’Œè€—æ—¶"""
        start_time = time.time()
        
        if tool_name == "rag":
            result = self.tools["rag"].execute(tool_input)
            elapsed = time.time() - start_time
            
            if result["success"]:
                return f"æ£€ç´¢åˆ° {len(result['documents'])} ç¯‡ç›¸å…³æ–‡æ¡£:\n{result['content'][:1000]}", elapsed
            else:
                return result["content"], elapsed
        
        elif tool_name == "calculator":
            result = self.tools["calculator"].execute(tool_input)
            elapsed = time.time() - start_time
            
            if result["success"]:
                return f"è®¡ç®—ç»“æœ: {result['result']}", elapsed
            else:
                return f"è®¡ç®—å¤±è´¥: {result['error']}", elapsed
        
        else:
            elapsed = time.time() - start_time
            return f"æœªçŸ¥å·¥å…·: {tool_name}", elapsed
    
    def generate_response(self, user_query: str) -> str:
        """ç”Ÿæˆ Agent å›ç­”ï¼ˆReAct å¾ªç¯ï¼‰"""
        total_start = time.time()
        
        print("\n" + "="*70)
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_query}")
        print("="*70)
        
        thought_action_history = []
        time_stats = {
            "thinking": 0,
            "tool_execution": 0,
            "total": 0
        }
        
        for iteration in range(self.max_iterations):
            iter_start = time.time()
            print(f"\nğŸ”„ [è¿­ä»£ {iteration + 1}/{self.max_iterations}]")
            
            # 1. æ„å»º Promptï¼ˆåŒ…å«å†å²å’ŒæŒ‡ä»¤ï¼‰
            system_prompt = """ä½ æ˜¯ä¸€ä¸ª AI ç ”ç©¶åŠ©æ‰‹ã€‚ä½ èƒ½è°ƒç”¨ä»¥ä¸‹å·¥å…·ï¼š

å·¥å…·åˆ—è¡¨ï¼š
1. rag: ä»çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯ã€‚æ ¼å¼: "rag(query)"
2. calculator: æ‰§è¡Œè®¡ç®—ã€‚æ ¼å¼: "calculator(expression)"

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š

Thought: ä½ å¯¹é—®é¢˜çš„æ€è€ƒï¼ˆåˆ†æé—®é¢˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼‰
Action: ä½ è¦è°ƒç”¨çš„å·¥å…·åç§°
Action Input: å·¥å…·çš„è¾“å…¥å‚æ•°
Observation: å·¥å…·çš„è¿”å›ç»“æœ

...ï¼ˆé‡å¤æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆï¼‰

Final Answer: åŸºäºæ‰€æœ‰è§‚å¯Ÿçš„æœ€ç»ˆå›ç­”"""

            # æ„å»ºå¯¹è¯
            prompt = f"""{system_prompt}

é—®é¢˜å†å²:
{chr(10).join(thought_action_history)}

å½“å‰é—®é¢˜: {user_query}

Thought:"""
            
            # 2. æ¨¡å‹ç”Ÿæˆæ€è€ƒå’Œè¡ŒåŠ¨
            think_start = time.time()
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=100,  # å‡å°‘åˆ° 100ï¼ˆä» 200ï¼‰
                temperature=0.5,     # é™ä½æ¸©åº¦ï¼Œæ›´å¿«æ”¶æ•›
                top_p=0.8,           # æ›´èšç„¦
                do_sample=True,
                repetition_penalty=1.2  # é˜²æ­¢é‡å¤
            )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            think_elapsed = time.time() - think_start
            time_stats["thinking"] += think_elapsed
            
            print(f"ğŸ¤” Thought:{response[:100]}...")
            print(f"   â±ï¸  æ€è€ƒè€—æ—¶: {think_elapsed*1000:.0f}ms")
            thought_action_history.append(f"Thought:{response}")
            
            # 3. è§£æ Action
            action, action_input = self.parse_action(response)
            
            if action is None:
                # æ²¡æœ‰æ‰¾åˆ° Actionï¼Œå¯èƒ½æ˜¯ Final Answer
                if "Final Answer:" in response:
                    final_answer = response.split("Final Answer:")[-1].strip()
                    total_elapsed = time.time() - total_start
                    
                    print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n{final_answer}")
                    print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
                    print(f"   â”‚")
                    print(f"   â”œâ”€ æ€è€ƒæ€»è€—æ—¶: {time_stats['thinking']*1000:.0f}ms")
                    print(f"   â”œâ”€ å·¥å…·æ€»è€—æ—¶: {time_stats['tool_execution']*1000:.0f}ms")
                    print(f"   â”œâ”€ æ€»è€—æ—¶: {total_elapsed:.2f}s")
                    print(f"   â””â”€ è¿­ä»£æ¬¡æ•°: {iteration + 1}")
                    
                    return final_answer
                else:
                    print(f"âš ï¸  æ— æ³•è§£æ Actionï¼Œé‡è¯•...")
                    continue
            
            print(f"ğŸ”§ Action: {action}")
            print(f"ğŸ“¥ Input: {action_input}")
            
            # 4. æ‰§è¡Œå·¥å…·
            observation, tool_elapsed = self.execute_tool(action, action_input)
            time_stats["tool_execution"] += tool_elapsed
            
            print(f"ğŸ‘ï¸  Observation: {observation[:200]}...")
            print(f"   â±ï¸  å·¥å…·è€—æ—¶: {tool_elapsed*1000:.0f}ms")
            
            thought_action_history.append(f"Action: {action}\nAction Input: {action_input}\nObservation: {observation}")
            
            iter_elapsed = time.time() - iter_start
            print(f"   â±ï¸  è¿­ä»£è€—æ—¶: {iter_elapsed:.2f}s")
            
            # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­
            if "Final Answer:" in response:
                final_answer = response.split("Final Answer:")[-1].strip()
                total_elapsed = time.time() - total_start
                
                print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n{final_answer}")
                print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
                print(f"   â”‚")
                print(f"   â”œâ”€ æ€è€ƒæ€»è€—æ—¶: {time_stats['thinking']*1000:.0f}ms")
                print(f"   â”œâ”€ å·¥å…·æ€»è€—æ—¶: {time_stats['tool_execution']*1000:.0f}ms")
                print(f"   â”œâ”€ æ€»è€—æ—¶: {total_elapsed:.2f}s")
                print(f"   â””â”€ è¿­ä»£æ¬¡æ•°: {iteration + 1}")
                
                return final_answer
        
        # è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
        total_elapsed = time.time() - total_start
        print(f"\nâš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°")
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   â”‚")
        print(f"   â”œâ”€ æ€è€ƒæ€»è€—æ—¶: {time_stats['thinking']*1000:.0f}ms")
        print(f"   â”œâ”€ å·¥å…·æ€»è€—æ—¶: {time_stats['tool_execution']*1000:.0f}ms")
        print(f"   â”œâ”€ æ€»è€—æ—¶: {total_elapsed:.2f}s")
        print(f"   â””â”€ è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
        
        return "æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜è¶…æ—¶ï¼Œè¯·ç®€åŒ–é—®é¢˜åé‡è¯•ã€‚"

# ======================== åˆå§‹åŒ– ========================
def load_qwen_model():
    """åŠ è½½ Qwen æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½ Qwen æ¨¡å‹...")
    load_start = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print(f"   âœ“ Tokenizer åŠ è½½å®Œæˆ")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    load_elapsed = time.time() - load_start
    print(f"   â±ï¸  æ¨¡å‹åŠ è½½è€—æ—¶: {load_elapsed:.2f}s")
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"   ğŸ’¾ æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
    
    return tokenizer, model

# ======================== ä¸»å‡½æ•° ========================
def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*70)
    print("ğŸš€ ReAct Agent ç³»ç»Ÿå¯åŠ¨")
    print("="*70)
    
    # åˆå§‹åŒ–
    tokenizer, model = load_qwen_model()
    agent = ReActAgent(tokenizer, model)
    
    print("\nâœ… Agent åˆå§‹åŒ–å®Œæ¯•ï¼")
    print("\nğŸ’¡ è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:\n")
    
    # äº¤äº’å¾ªç¯
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() == 'quit':
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # Agent ç”Ÿæˆå›ç­”
            response = agent.generate_response(user_input)
            print(f"\nğŸ¤– Agent: {response}\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()