"""
RAG è¯Šæ–­å·¥å…· (rag_debug.py)
- æŸ¥çœ‹çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
- çœ‹çœ‹"aiå­¦ä¹ è·¯å¾„"çš„å†…å®¹æ˜¯å¦è¢«æ­£ç¡®åŠ è½½
- æµ‹è¯•å‘é‡æœç´¢ç»“æœ
"""

import os
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

FAISS_INDEX_PATH = r"D:\HF_models\faiss_index"
KNOWLEDGE_BASE_PATH = r"D:\HF_models\knowledge_base"

def diagnose():
    """è¯Šæ–­å‡½æ•°"""
    print("="*70)
    print("ğŸ” RAG è¯Šæ–­å·¥å…·")
    print("="*70)
    
    # 1. æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶
    print("\n1ï¸âƒ£  çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨:")
    print("-" * 70)
    kb_path = Path(KNOWLEDGE_BASE_PATH)
    pdf_files = list(kb_path.glob("*.pdf"))
    
    print(f"æ€»è®¡ï¼š{len(pdf_files)} ä¸ª PDF æ–‡ä»¶\n")
    for i, pdf in enumerate(sorted(pdf_files), 1):
        size_mb = pdf.stat().st_size / 1e6
        print(f"{i:2}. ã€Š{pdf.stem}ã€‹ ({size_mb:.1f}MB)")
    
    # 2. æ£€æŸ¥æ˜¯å¦æœ‰"å­¦ä¹ è·¯å¾„"
    print("\n2ï¸âƒ£  æœç´¢ 'å­¦ä¹ è·¯å¾„' ç›¸å…³æ–‡ä»¶:")
    print("-" * 70)
    learning_files = [f for f in pdf_files if "å­¦ä¹ " in f.stem or "è·¯å¾„" in f.stem]
    if learning_files:
        for f in learning_files:
            print(f"âœ… æ‰¾åˆ°: {f.stem}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°åŒ…å« 'å­¦ä¹ ' æˆ– 'è·¯å¾„' çš„æ–‡ä»¶")
    
    # 3. åŠ è½½å‘é‡åº“å¹¶æ£€æŸ¥
    print("\n3ï¸âƒ£  å‘é‡åº“ç»Ÿè®¡:")
    print("-" * 70)
    
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        model_kwargs={"device": "cuda:0"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    try:
        vector_store = FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
        print(f"âœ… å‘é‡åº“åŠ è½½æˆåŠŸ")
        print(f"   å‘é‡æ•°: {vector_store.index.ntotal}")
    except Exception as e:
        print(f"âŒ å‘é‡åº“åŠ è½½å¤±è´¥: {e}")
        return
    
    # 4. æµ‹è¯•æœç´¢
    print("\n4ï¸âƒ£  æµ‹è¯•å‘é‡æœç´¢:")
    print("-" * 70)
    
    test_queries = [
        "aiå­¦ä¹ è·¯å¾„",
        "å­¦ä¹ ai",
        "aiåŸºç¡€",
        "æ ¸å¿ƒæ¦‚å¿µ",
        "æ‰«ç›²",
        "æ·±åº¦å­¦ä¹ "
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        docs = vector_store.similarity_search(query, k=5)
        
        for j, doc in enumerate(docs, 1):
            filename = doc.metadata.get('filename', 'Unknown')
            content = doc.page_content[:60].replace('\n', ' ')
            print(f"   {j}. ã€Š{filename}ã€‹")
            print(f"      {content}...")
    
    # 5. æ£€æŸ¥ç‰¹å®šæ–‡ä»¶æ˜¯å¦åœ¨å‘é‡åº“ä¸­
    print("\n5ï¸âƒ£  æ£€æŸ¥ 'aiå­¦ä¹ è·¯å¾„' æ˜¯å¦åœ¨å‘é‡åº“ä¸­:")
    print("-" * 70)
    
    # é€šè¿‡æœç´¢æ¥éªŒè¯
    docs = vector_store.similarity_search("aiå­¦ä¹ è·¯å¾„ æ•™ç¨‹ å…¥é—¨ æ­¥éª¤", k=10)
    filenames = set([doc.metadata.get('filename', '') for doc in docs])
    
    print(f"æœç´¢åˆ°çš„æ–‡ä»¶ï¼ˆå»é‡ï¼‰: {len(filenames)} ä¸ª")
    for fname in sorted(filenames):
        if fname:
            print(f"  - {fname}")
    
    if "aiå­¦ä¹ è·¯å¾„" in str(filenames):
        print(f"\nâœ… 'aiå­¦ä¹ è·¯å¾„' åœ¨å‘é‡åº“ä¸­")
    else:
        print(f"\nâš ï¸  'aiå­¦ä¹ è·¯å¾„' å¯èƒ½ä¸åœ¨å‘é‡åº“ä¸­")
        print(f"   å¯èƒ½åŸå› ï¼š")
        print(f"   1. æ–‡ä»¶è¢«åˆ é™¤æˆ–ç§»åŠ¨")
        print(f"   2. FAISS ç´¢å¼•æ˜¯ç”¨æ—§çš„æ–‡ä»¶æ„å»ºçš„")
        print(f"   3. éœ€è¦é‡æ–°æ„å»ºå‘é‡åº“")

if __name__ == "__main__":
    diagnose()