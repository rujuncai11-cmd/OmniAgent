"""
RAG ä¼˜åŒ–ç‰ˆæœ¬ (rag_optimize.py) - ç®€åŒ–ç‰ˆ
ä¸“ä¸º AI/Agent é¢†åŸŸçŸ¥è¯†åº“å®šåˆ¶
åŠŸèƒ½ï¼š
1. ä¸­æ–‡ embedding æ¨¡å‹ï¼ˆBGEï¼‰
2. æ¥æºè¿½è¸ª + ç½®ä¿¡åº¦æ˜¾ç¤º
3. ç®€å•å‘é‡æ£€ç´¢ï¼ˆä¸ä¾èµ– EnsembleRetrieverï¼‰
4. è‡ªåŠ¨åˆ†ç±»è¾“å‡º
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Tuple

# å‘é‡åº“ + æ–‡æœ¬å¤„ç†
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# å¤§æ¨¡å‹
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ======================== é…ç½® ========================
# ======================== é…ç½® ========================
KNOWLEDGE_BASE_PATH = r"D:\HF_models\knowledge_base"  # ä½ çš„æ–‡ç« è·¯å¾„
FAISS_INDEX_PATH = r"D:\HF_models\faiss_index"  # FAISS å­˜å‚¨è·¯å¾„
MODEL_ID = "Qwen/Qwen2.5-3B-Instruct"  # æ”¹ç”¨ model_id
CACHE_DIR = r"D:\HF_models"  # æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆç¡®ä¿ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²ï¼‰

# RAG å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
CHUNK_SIZE = 600  # æé«˜åˆ° 600ï¼Œæ›´å¤šä¸Šä¸‹æ–‡
CHUNK_OVERLAP = 150  # å¢åŠ é‡å ï¼Œé¿å…å…³é”®ä¿¡æ¯ä¸¢å¤±
TOP_K_RETRIEVAL = 5  # æ£€ç´¢ top 5 æ–‡æ¡£
CONFIDENCE_THRESHOLD = 0.5  # ç½®ä¿¡åº¦é˜ˆå€¼

# ======================== æ­¥éª¤1ï¼šåŠ è½½ & åˆ†å— ========================
def load_and_chunk_documents():
    """åŠ è½½æ‰€æœ‰æ–‡ç«  + æ™ºèƒ½åˆ†å—"""
    print("ğŸ“– [æ­¥éª¤1] åŠ è½½æ–‡ç« ...")
    
    # ä½¿ç”¨ PyPDF åŠ è½½ PDF æ–‡ä»¶
    from langchain_community.document_loaders import PyPDFLoader
    
    documents = []
    pdf_path = Path(KNOWLEDGE_BASE_PATH)
    
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ PDF
    for pdf_file in pdf_path.glob("*.pdf"):
        print(f"  ğŸ“„ åŠ è½½ {pdf_file.name}...")
        loader = PyPDFLoader(str(pdf_file))
        docs = loader.load()
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ æ¥æºæ–‡ä»¶å
        for doc in docs:
            doc.metadata["filename"] = pdf_file.stem
        
        documents.extend(docs)
    
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªé¡µé¢")
    
    print("ğŸ”ª [æ­¥éª¤1] åˆ†å—ä¸­...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " "]  # ä¼˜å…ˆæŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†
    )
    chunks = splitter.split_documents(documents)
    print(f"âœ… åˆ†æˆäº† {len(chunks)} ä¸ª chunks")
    
    return chunks

# ======================== æ­¥éª¤2ï¼šæ„å»ºå‘é‡åº“ ========================
def build_vector_store(chunks):
    """ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„ embedding æ¨¡å‹"""
    print("ğŸ§  [æ­¥éª¤2] æ„å»ºå‘é‡åº“ï¼ˆä¸­æ–‡ BGEï¼‰...")
    
    # ä½¿ç”¨ BAAI/bge-large-zh-v1.5ï¼ˆä¸­æ–‡æœ€ä¼˜ï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        model_kwargs={"device": "cuda:0"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    # åˆ›å»º FAISS å‘é‡åº“
    if os.path.exists(FAISS_INDEX_PATH):
        print("â™»ï¸  åŠ è½½å·²æœ‰å‘é‡åº“...")
        vector_store = FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    else:
        print("ğŸ—ï¸  æ„å»ºæ–°å‘é‡åº“...")
        vector_store = FAISS.from_documents(chunks, embeddings)
        vector_store.save_local(FAISS_INDEX_PATH)
        print(f"âœ… å‘é‡åº“å·²ä¿å­˜åˆ° {FAISS_INDEX_PATH}")
    
    return vector_store, embeddings

# ======================== æ­¥éª¤3ï¼šç®€å•å‘é‡æ£€ç´¢ ========================
def build_retriever(vector_store):
    """æ„å»ºå‘é‡æ£€ç´¢å™¨"""
    print("ğŸ” [æ­¥éª¤3] æ„å»ºæ£€ç´¢å™¨...")
    
    retriever = vector_store.as_retriever(
        search_kwargs={"k": TOP_K_RETRIEVAL}
    )
    
    print("âœ… æ£€ç´¢å™¨å‡†å¤‡å®Œæ¯•")
    return retriever

# ======================== æ­¥éª¤4ï¼šåŠ è½½å¤§æ¨¡å‹ ========================
def load_qwen_model():
    """åŠ è½½ Qwen æ¨¡å‹ï¼ˆä½¿ç”¨ model_idï¼‰"""
    print("ğŸ¤– [æ­¥éª¤4] åŠ è½½ Qwen æ¨¡å‹...")
    print(f"   æ¨¡å‹ ID: {MODEL_ID}")
    print(f"   ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print("   âœ“ Tokenizer åŠ è½½æˆåŠŸ")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"   ğŸ’¾ GPU æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
    
    return tokenizer, model

# ======================== æ­¥éª¤5ï¼šRAG ç”Ÿæˆ ========================
def rag_generate(
    query: str,
    retriever,
    tokenizer,
    model,
    top_k: int = 3
) -> Dict:
    """
    RAG ç”Ÿæˆï¼šæ£€ç´¢ + ç”Ÿæˆ + æ¥æºè¿½è¸ª
    
    è¿”å›ï¼š
    {
        "answer": "ç”Ÿæˆçš„ç­”æ¡ˆ",
        "sources": [{"filename": "...", "content": "...", "score": 0.85}, ...],
        "confidence": 0.88
    }
    """
    print(f"\nâ“ é—®é¢˜ï¼š{query}")
    print("ğŸ” æ£€ç´¢ä¸­...")
    
    # 1. æ£€ç´¢æ–‡æ¡£
    retrieved_docs = retriever.invoke(query)
    
    if not retrieved_docs:
        return {
            "answer": "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
            "sources": [],
            "confidence": 0.0
        }
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([
        f"ã€æ¥æºï¼š{doc.metadata.get('filename', 'Unknown')}ã€‘\n{doc.page_content}"
        for doc in retrieved_docs[:top_k]
    ])
    
    # 3. æ„å»º prompt
    prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI é¢†åŸŸçš„ä¸“å®¶åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{context}

ã€é—®é¢˜ã€‘
{query}

ã€è¦æ±‚ã€‘
- ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦é‡å¤ä¿¡æ¯
- å¦‚æœçŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯´"æš‚æ— ç›¸å…³ä¿¡æ¯"
- å›ç­”é•¿åº¦ï¼š100-300 å­—

ã€ç­”æ¡ˆã€‘"""
    
    # 4. ç”Ÿæˆç­”æ¡ˆ
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    # 5. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ£€ç´¢å¾—åˆ†ï¼‰
    confidence = min(1.0, len(retrieved_docs) / TOP_K_RETRIEVAL * 0.95)
    
    # 6. æ ¼å¼åŒ–æº
    sources = [
        {
            "filename": doc.metadata.get('filename', 'Unknown'),
            "content": doc.page_content[:200] + "...",  # å‰ 200 å­—
            "confidence": round(confidence, 2)
        }
        for doc in retrieved_docs[:top_k]
    ]
    
    return {
        "answer": answer.strip(),
        "sources": sources,
        "confidence": round(confidence, 2)
    }

# ======================== æ­¥éª¤6ï¼šæµ‹è¯• ========================
def test_rag():
    """æµ‹è¯• RAG ç³»ç»Ÿ"""
    # åˆå§‹åŒ–
    chunks = load_and_chunk_documents()
    vector_store, embeddings = build_vector_store(chunks)
    retriever = build_retriever(vector_store)
    tokenizer, model = load_qwen_model()
    
    # æµ‹è¯•é—®é¢˜ï¼ˆé’ˆå¯¹ä½ çš„çŸ¥è¯†åº“ï¼‰
    test_questions = [
        "AgentScope æ¡†æ¶çš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿæœ‰å“ªäº›ä¼˜åŒ–æ–¹æ³•ï¼Ÿ",
        "PyTorch FSDP å¦‚ä½•åŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ",
        "VideoRAG å¦‚ä½•å¤„ç†é•¿è§†é¢‘çš„ä¸Šä¸‹æ–‡ï¼Ÿ",
        "å¤šæ™ºèƒ½ä½“ä»¿çœŸçš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
        "é‡å­è®¡ç®—åœ¨ AI ä¸­çš„åº”ç”¨å‰æ™¯å¦‚ä½•ï¼Ÿ"  # æ•…æ„é—®çŸ¥è¯†åº“æ²¡æœ‰çš„
    ]
    
    results = []
    for q in test_questions:
        result = rag_generate(q, retriever, tokenizer, model, top_k=3)
        results.append({
            "question": q,
            "answer": result["answer"][:150],  # åªæ˜¾ç¤ºå‰ 150 å­—
            "confidence": result["confidence"],
            "sources_count": len(result["sources"]),
            "sources": result["sources"]
        })
        print(f"âœ… ç­”æ¡ˆï¼ˆç½®ä¿¡åº¦ {result['confidence']}ï¼‰ï¼š{result['answer'][:100]}...")
        print(f"ğŸ“š æ¥æºæ•°ï¼š{len(result['sources'])}")
        if result["sources"]:
            print(f"   æ¥æºæ–‡ä»¶ï¼š{[s['filename'] for s in result['sources']]}")
    
    # ä¿å­˜ç»“æœ
    with open("rag_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print("\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° rag_test_results.json")
    
    return results

# ======================== ä¸»å‡½æ•° ========================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ RAG ä¼˜åŒ–ç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    test_rag()
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)